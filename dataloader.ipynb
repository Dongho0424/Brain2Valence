{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dongho/anaconda3/envs/temp/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import utils\n",
    "import webdataset as wds\n",
    "import os\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # load package\n",
    "# import numpy as np\n",
    "# import scipy.io\n",
    "\n",
    "# file_name_nsd_stim = './nsd_stim_info_merged.csv'\n",
    "# file_name_emotic_annot = './emotic_annotations.mat'\n",
    "\n",
    "# ## get EMOTIC data\n",
    "# data = scipy.io.loadmat(file_name_emotic_annot, simplify_cells=True)\n",
    "# emotic_data = data['train'] + data['test'] + data['val']\n",
    "# emotic_coco_data = [x for x in emotic_data if x['original_database']['name']=='mscoco']\n",
    "# coco_id = [x['original_database']['info']['image_id'] for x in emotic_coco_data]\n",
    "# annotations = [x['person'] for x in emotic_coco_data] \n",
    "# emotic_annotations = []\n",
    "# for annot in annotations:\n",
    "#     annot = [annot] if type(annot)==dict else annot\n",
    "\n",
    "#     valence = []; arousal = []; dominance = []\n",
    "#     for person in annot:\n",
    "#         person = person['annotations_continuous']\n",
    "#         person = [person] if type(person)==dict else person\n",
    "#         valence += [np.mean([x['valence'] for x in person])]\n",
    "#         arousal += [np.mean([x['arousal'] for x in person])]\n",
    "#         dominance += [np.mean([x['dominance'] for x in person])]\n",
    "#     emotic_annotations += [{ 'valence':valence, 'arousal':arousal, 'dominance':dominance}]\n",
    "\n",
    "# emotic_annotations = dict(zip(coco_id, emotic_annotations))\n",
    "\n",
    "# # define function\n",
    "# get_emotic_annot       = lambda coco_id, metric_type: np.mean(emotic_annotations[coco_id][metric_type])\n",
    "# get_emotic_annot_indiv = lambda coco_id, metric_type: emotic_annotations[coco_id][metric_type]\n",
    "\n",
    "# ## get NSD data\n",
    "# df = data = pd.read_csv(file_name_nsd_stim)\n",
    "# nsd_id = df['Unnamed: 0'].values\n",
    "# nsd_cocoid = df['cocoId'].values\n",
    "# nsd_cocosplit = df['cocoSplit'].values\n",
    "# nsd_isshared = df['shared1000'].values\n",
    "\n",
    "\n",
    "# ## target NSD data\n",
    "# joint_cocoid = nsd_cocoid[np.isin(nsd_cocoid, list(emotic_annotations.keys()))]\n",
    "# # print(target_cocoid)\n",
    "# print(\"The Num of intersection between NSD and EMOTIC dataSet:\", len(joint_cocoid))\n",
    "\n",
    "# \"\"\"\n",
    "# 1. TrainSet: NSD 데이터셋에서 개별적으로 보여준 것\n",
    "# 2. TestSet: NSD 데이터셋에서 공통으로 보여준 것\n",
    "# 3. 공통으로, 사람 한명 있는 데이터 사용\n",
    "#  - `len(person)==1`\n",
    "#  \"\"\"\n",
    "# # 사람 한명 있는 데이터\n",
    "# target_cocoid = [coco_id for coco_id in joint_cocoid if len(get_emotic_annot_indiv(coco_id, 'valence')) == 1]\n",
    "# print(\"The number of joint pictures which reveals only one person:\", len(target_cocoid))\n",
    "# train_cocoid = nsd_cocoid[np.isin(nsd_cocoid, target_cocoid) &  ~nsd_isshared]\n",
    "# print(f\"cocoid for train, #: {len(train_cocoid)}\")\n",
    "# test_cocoid = nsd_cocoid[np.isin(nsd_cocoid, target_cocoid) &  nsd_isshared]\n",
    "# print(f\"cocoid for test, #: {len(test_cocoid)}\")\n",
    "\n",
    "# # data for common image shown to all subjects\n",
    "# # regarding as test set\n",
    "# test_df = df[df[['subject1', 'subject2', 'subject3', 'subject4', 'subject5', 'subject6', 'subject7', 'subject8']].all(axis=1)]\n",
    "# train_df =  df[~df[['subject1', 'subject2', 'subject3', 'subject4', 'subject5', 'subject6', 'subject7', 'subject8']].all(axis=1)]\n",
    "\n",
    "# for i in range(1, 9):\n",
    "#     current_subject = f\"subject{i}\"\n",
    "#     curr_df = train_df[train_df[current_subject] == True] # filter only shown to curr_subject\n",
    "\n",
    "#     # 현재 subject의 데이터에서 train_cocoid와 겹치는 데이터 찾기\n",
    "#     # 즉 현재 subject에 보여준 사진 중 cocoid가 EMOTIC dataset에 사용되었던 것 필터링\n",
    "#     target_train_df = curr_df[np.isin(curr_df[\"cocoId\"], train_cocoid)]\n",
    "#     print(f\"data of {current_subject}: {target_train_df.shape}\")\n",
    "\n",
    "# target_test_df = test_df[np.isin(test_df[\"cocoId\"], test_cocoid)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_brain3d(brain_3d, target_shape=(96, 96, 96)):\n",
    "    # Initialize pad_width\n",
    "    pad_width = [(0, 0)] * 3  # For 3D brain_3day\n",
    "\n",
    "    # Calculate padding needed for each dimension\n",
    "    for i in range(3):\n",
    "        current_size = brain_3d.shape[i]\n",
    "        if current_size < target_shape[i]:\n",
    "            # Calculate padding\n",
    "            total_pad = target_shape[i] - current_size\n",
    "            pad_before = total_pad // 2\n",
    "            pad_after = total_pad - pad_before\n",
    "            pad_width[i] = (pad_before, pad_after)\n",
    "\n",
    "    # Apply padding\n",
    "    brain_3d = np.pad(brain_3d, pad_width=pad_width, mode='constant', constant_values=0)\n",
    "\n",
    "    # Apply truncation if necessary\n",
    "    brain_3d = brain_3d[:target_shape[0], :target_shape[1], :target_shape[2]]\n",
    "\n",
    "    # print(\"reshaped brain_3d shape\", brain_3d.shape)\n",
    "    return brain_3d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import scipy\n",
    "\n",
    "# class BrainValenceDataset(Dataset):\n",
    "#     def __init__(self, data_path, split, subj):\n",
    "#         self.data_path = data_path\n",
    "#         self.split = split # train, val, test\n",
    "#         self.subj = subj\n",
    "#         self.metadata = pd.read_csv(os.path.join(self.data_path, f'{self.split}_subj0{self.subj}_metadata.csv'))\n",
    "\n",
    "#         ## get joint data between NSD and EMOTIC and COCO\n",
    "#         self.emotic_annotations = self.get_emotic_data()\n",
    "#         self.target_cocoid = self.get_NSD_data(self.emotic_annotations)\n",
    "\n",
    "#         ## use only joint and target(one person in picture) image\n",
    "#         self.valid_indices = []\n",
    "#         for idx in range(len(self.metadata)):\n",
    "#             sample = self.metadata.iloc[idx]\n",
    "#             cocoid = torch.from_numpy(np.load(os.path.join(self.data_path, self.split, sample['coco'])))\n",
    "#             if cocoid in self.target_cocoid:\n",
    "#                 self.valid_indices.append(idx)\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.valid_indices)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         actual_idx = self.valid_indices[idx]\n",
    "\n",
    "#         sample = self.metadata.iloc[actual_idx]\n",
    "#         cocoid = torch.from_numpy(np.load(os.path.join(self.data_path, self.split, sample['coco'])))\n",
    "#         brain_3d = torch.from_numpy(np.load(os.path.join(self.data_path, self.split, sample['mri']))) # (3, *, *, *)\n",
    "#         brain_3d = np.mean(brain_3d, axis=0) # (*, *, *)\n",
    "#         brain_3d = self.reshape_brain3d(brain_3d, target_shape=(96, 96, 96)) # (96, 96, 96)\n",
    "        \n",
    "#         valence = np.mean(self.emotic_annotations[cocoid]['valence'])\n",
    "\n",
    "#         return cocoid, brain_3d, valence\n",
    "    \n",
    "#     def reshape_brain3d(brain_3d, target_shape=(96, 96, 96)):\n",
    "#         # Initialize pad_width\n",
    "#         pad_width = [(0, 0)] * 3  # For 3D brain_3day\n",
    "\n",
    "#         # Calculate padding needed for each dimension\n",
    "#         for i in range(3):\n",
    "#             current_size = brain_3d.shape[i]\n",
    "#             if current_size < target_shape[i]:\n",
    "#                 # Calculate padding\n",
    "#                 total_pad = target_shape[i] - current_size\n",
    "#                 pad_before = total_pad // 2\n",
    "#                 pad_after = total_pad - pad_before\n",
    "#                 pad_width[i] = (pad_before, pad_after)\n",
    "\n",
    "#         # Apply padding\n",
    "#         brain_3d = np.pad(brain_3d, pad_width=pad_width, mode='constant', constant_values=0)\n",
    "\n",
    "#         # Apply truncation if necessary\n",
    "#         brain_3d = brain_3d[:target_shape[0], :target_shape[1], :target_shape[2]]\n",
    "\n",
    "#         return brain_3d\n",
    "    \n",
    "#     def get_emotic_data() -> dict:\n",
    "#         file_name_emotic_annot = './emotic_annotations.mat'\n",
    "\n",
    "#         ## get EMOTIC data\n",
    "#         data = scipy.io.loadmat(file_name_emotic_annot, simplify_cells=True)\n",
    "#         emotic_data = data['train'] + data['test'] + data['val']\n",
    "#         emotic_coco_data = [x for x in emotic_data if x['original_database']['name']=='mscoco']\n",
    "#         coco_id = [x['original_database']['info']['image_id'] for x in emotic_coco_data]\n",
    "#         annotations = [x['person'] for x in emotic_coco_data] \n",
    "#         emotic_annotations = []\n",
    "#         for annot in annotations:\n",
    "#             annot = [annot] if type(annot)==dict else annot\n",
    "\n",
    "#             valence = []; arousal = []; dominance = []\n",
    "#             for person in annot:\n",
    "#                 person = person['annotations_continuous']\n",
    "#                 person = [person] if type(person)==dict else person\n",
    "#                 valence += [np.mean([x['valence'] for x in person])]\n",
    "#                 arousal += [np.mean([x['arousal'] for x in person])]\n",
    "#                 dominance += [np.mean([x['dominance'] for x in person])]\n",
    "#             emotic_annotations += [{ 'valence':valence, 'arousal':arousal, 'dominance':dominance}]\n",
    "\n",
    "#         emotic_annotations = dict(zip(coco_id, emotic_annotations))\n",
    "\n",
    "#         return emotic_annotations\n",
    "\n",
    "#     def get_NSD_data(emotic_annotations):\n",
    "#         # out: target_cocoid\n",
    "#         file_name_nsd_stim = './nsd_stim_info_merged.csv'\n",
    "\n",
    "#         ## get NSD data\n",
    "#         df = data = pd.read_csv(file_name_nsd_stim)\n",
    "#         nsd_id = df['Unnamed: 0'].values\n",
    "#         nsd_cocoid = df['cocoId'].values\n",
    "#         nsd_cocosplit = df['cocoSplit'].values\n",
    "#         nsd_isshared = df['shared1000'].values\n",
    "\n",
    "#         joint_cocoid = nsd_cocoid[np.isin(nsd_cocoid, list(emotic_annotations.keys()))]\n",
    "#         target_cocoid = [coco_id for coco_id in joint_cocoid if len(emotic_annotations[coco_id]['valence']) == 1]\n",
    "#         train_cocoid = nsd_cocoid[np.isin(nsd_cocoid, target_cocoid) &  ~nsd_isshared]\n",
    "#         test_cocoid = nsd_cocoid[np.isin(nsd_cocoid, target_cocoid) &  nsd_isshared]\n",
    "\n",
    "#         # data for common image shown to all subjects\n",
    "#         # regarding as test set\n",
    "#         # test_df = df[df[['subject1', 'subject2', 'subject3', 'subject4', 'subject5', 'subject6', 'subject7', 'subject8']].all(axis=1)]\n",
    "#         # train_df = df[~df[['subject1', 'subject2', 'subject3', 'subject4', 'subject5', 'subject6', 'subject7', 'subject8']].all(axis=1)]\n",
    "\n",
    "#         return target_cocoid\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "    \n",
    "def get_emotic_data() -> dict:\n",
    "    file_name_emotic_annot = './emotic_annotations.mat'\n",
    "\n",
    "    ## get EMOTIC data\n",
    "    data = scipy.io.loadmat(file_name_emotic_annot, simplify_cells=True)\n",
    "    emotic_data = data['train'] + data['test'] + data['val']\n",
    "    emotic_coco_data = [x for x in emotic_data if x['original_database']['name']=='mscoco']\n",
    "    coco_id = [x['original_database']['info']['image_id'] for x in emotic_coco_data]\n",
    "    annotations = [x['person'] for x in emotic_coco_data] \n",
    "    emotic_annotations = []\n",
    "    for annot in annotations:\n",
    "        annot = [annot] if type(annot)==dict else annot\n",
    "\n",
    "        valence = []; arousal = []; dominance = []\n",
    "        for person in annot:\n",
    "            person = person['annotations_continuous']\n",
    "            person = [person] if type(person)==dict else person\n",
    "            valence += [np.mean([x['valence'] for x in person])]\n",
    "            arousal += [np.mean([x['arousal'] for x in person])]\n",
    "            dominance += [np.mean([x['dominance'] for x in person])]\n",
    "        emotic_annotations += [{ 'valence':valence, 'arousal':arousal, 'dominance':dominance}]\n",
    "\n",
    "    emotic_annotations = dict(zip(coco_id, emotic_annotations))\n",
    "\n",
    "    return emotic_annotations\n",
    "\n",
    "def get_NSD_data(emotic_annotations):\n",
    "    # out: target_cocoid\n",
    "    file_name_nsd_stim = './nsd_stim_info_merged.csv'\n",
    "\n",
    "    ## get NSD data\n",
    "    df = pd.read_csv(file_name_nsd_stim)\n",
    "    nsd_id = df['nsdId'].values\n",
    "    nsd_cocoid = df['cocoId'].values\n",
    "    nsd_cocosplit = df['cocoSplit'].values\n",
    "    nsd_isshared = df['shared1000'].values\n",
    "\n",
    "    joint_cocoid = nsd_cocoid[np.isin(nsd_cocoid, list(emotic_annotations.keys()))]\n",
    "    target_cocoid = [coco_id for coco_id in joint_cocoid if len(emotic_annotations[coco_id]['valence']) == 1]\n",
    "    train_cocoid = nsd_cocoid[np.isin(nsd_cocoid, target_cocoid) &  ~nsd_isshared]\n",
    "    test_cocoid = nsd_cocoid[np.isin(nsd_cocoid, target_cocoid) &  nsd_isshared]\n",
    "\n",
    "    return df, target_cocoid\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotic_annotations = get_emotic_data()\n",
    "df, target_cocoid = get_NSD_data(emotic_annotations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cautions\n",
    "- metadata에 해당하는 data의 coco가 coco_id를 말하는 것이 아니라, \n",
    "- `nsd_stim_info_merged.csv` 파일의 인덱스를 뜻하는 거였음.....\n",
    "- 그래서 coco -> coco_id 변환용 Dataframe 및 함수가 필요함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1200\n"
     ]
    }
   ],
   "source": [
    "data_path=\"/home/juhyeon/fsx/proj-medarc/fmri/natural-scenes-dataset/webdataset_avg_split\"\n",
    "split=\"val\"\n",
    "subjects=[1, 2, 5, 7]\n",
    "\n",
    "dfs = [pd.read_csv(os.path.join(data_path, f'{split}_subj0{subj}_metadata.csv')) for subj in subjects]\n",
    "metadata = pd.concat(dfs)\n",
    "metadata.reset_index(inplace=True, drop=True)\n",
    "print(len(metadata))\n",
    "\n",
    "nsd_id = metadata['coco'].apply(lambda x: np.load(data_path+f\"/{split}/\"+x)[-1])\n",
    "# coco_id = df[df['nsdId'].isin(nsd_id)]['cocoId']\n",
    "coco_id = []\n",
    "isin = nsd_id.isin(df['nsdId'])#; print(isin)\n",
    "for i, x in nsd_id.items():\n",
    "    if isin[i]: coco_id.append(df.loc[x, 'cocoId'])\n",
    "\n",
    "coco_id = pd.Series(coco_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BrainValenceDataset(Dataset):\n",
    "    def __init__(self,  data_path, split, emotic_annotations, nsd_df, target_cocoid, subjects=[1, 2, 5, 7]):\n",
    "        self.data_path = data_path\n",
    "        self.split = split # train, val, test\n",
    "        self.subjects = subjects # [1, 2, 5, 7]\n",
    "        if split in ['train', 'val']:\n",
    "            dfs = [pd.read_csv(os.path.join(self.data_path, f'{self.split}_subj0{subj}_metadata.csv')) for subj in self.subjects]\n",
    "            self.metadata = pd.concat(dfs)\n",
    "        elif split == 'test':\n",
    "            # As test data is all same to every subjects,\n",
    "            df = pd.read_csv(os.path.join(self.data_path, 'test_subj01_metadata.csv'))\n",
    "            self.metadata = df\n",
    "        else: IndexError(\"Wrong split type\")\n",
    "        \n",
    "        self.metadata.reset_index(inplace=True, drop=True)       \n",
    "\n",
    "        ## get joint data between NSD and EMOTIC and COCO\n",
    "        self.nsd_df = nsd_df\n",
    "        self.emotic_annotations = emotic_annotations\n",
    "        self.target_cocoid = target_cocoid\n",
    "\n",
    "        # pre convert nsd data appropriately\n",
    "        self.coco_id = self.nsd2coco()\n",
    "\n",
    "        ## use only joint and target(one person in picture) image\n",
    "        # target_cocoid와 joint 한 것만 남긴다.\n",
    "        isin = self.coco_id.isin(self.target_cocoid)\n",
    "        self.coco_id = self.coco_id[isin]\n",
    "        # metadata도 남길 애들만 남긴다.\n",
    "        self.metadata = self.metadata[isin]\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.metadata)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        sample = self.metadata.iloc[idx]\n",
    "        id = self.coco_id.iloc[idx]\n",
    "        brain_3d = torch.from_numpy(np.load(os.path.join(self.data_path, self.split, sample['mri']))) # (3, *, *, *)\n",
    "        brain_3d = np.mean(brain_3d, axis=0) # (*, *, *)\n",
    "        brain_3d = self.reshape_brain3d(brain_3d, target_shape=(96, 96, 96)) # (96, 96, 96)\n",
    "        \n",
    "        valence = np.mean(self.emotic_annotations[id]['valence'])\n",
    "\n",
    "        return brain_3d, valence\n",
    "\n",
    "    def nsd2coco(self) -> pd.Series :\n",
    "        # metadata.csv의 coco column은 nsd_id이므로, 이를 `nsd_stim_info_merged.csv` 를 읽어온 후, \n",
    "        # nsdid => coco id 로 바꿔줌\n",
    "\n",
    "        nsd_id = self.metadata['coco'].apply(lambda x: np.load(self.data_path+f\"/{self.split}/\"+x)[-1])\n",
    "        coco_id = []\n",
    "        isin = nsd_id.isin(self.nsd_df['nsdId'])\n",
    "        for i, x in nsd_id.items():\n",
    "            if isin[i]: coco_id.append(self.nsd_df.loc[x, 'cocoId'])\n",
    "\n",
    "        coco_id = pd.Series(coco_id)\n",
    "\n",
    "        return coco_id\n",
    "    \n",
    "    def reshape_brain3d(brain_3d, target_shape=(96, 96, 96)):\n",
    "        # Initialize pad_width\n",
    "        pad_width = [(0, 0)] * 3  # For 3D brain_3day\n",
    "\n",
    "        # Calculate padding needed for each dimension\n",
    "        for i in range(3):\n",
    "            current_size = brain_3d.shape[i]\n",
    "            if current_size < target_shape[i]:\n",
    "                # Calculate padding\n",
    "                total_pad = target_shape[i] - current_size\n",
    "                pad_before = total_pad // 2\n",
    "                pad_after = total_pad - pad_before\n",
    "                pad_width[i] = (pad_before, pad_after)\n",
    "\n",
    "        # Apply padding\n",
    "        brain_3d = np.pad(brain_3d, pad_width=pad_width, mode='constant', constant_values=0)\n",
    "\n",
    "        # Apply truncation if necessary\n",
    "        brain_3d = brain_3d[:target_shape[0], :target_shape[1], :target_shape[2]]\n",
    "\n",
    "        return brain_3d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3582"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = BrainValenceDataset(\n",
    "    data_path=\"/home/juhyeon/fsx/proj-medarc/fmri/natural-scenes-dataset/webdataset_avg_split\",\n",
    "    split=\"train\",\n",
    "    emotic_annotations=emotic_annotations,\n",
    "    nsd_df=df,\n",
    "    target_cocoid=target_cocoid,\n",
    "    subjects=[1, 2, 5, 7]\n",
    ")\n",
    "\n",
    "len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "88"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_dataset = BrainValenceDataset(\n",
    "    data_path=\"/home/juhyeon/fsx/proj-medarc/fmri/natural-scenes-dataset/webdataset_avg_split\",\n",
    "    split=\"val\",\n",
    "    emotic_annotations=emotic_annotations,\n",
    "    nsd_df=df,\n",
    "    target_cocoid=target_cocoid,\n",
    "    subjects=[1, 2, 5, 7]\n",
    ")\n",
    "\n",
    "len(val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "110"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset = BrainValenceDataset(\n",
    "    data_path=\"/home/juhyeon/fsx/proj-medarc/fmri/natural-scenes-dataset/webdataset_avg_split\",\n",
    "    split=\"test\",\n",
    "    emotic_annotations=emotic_annotations,\n",
    "    nsd_df=df,\n",
    "    target_cocoid=target_cocoid,\n",
    "    subjects=[1, 2, 5, 7]\n",
    ")\n",
    "\n",
    "len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "temp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
