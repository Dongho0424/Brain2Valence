{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dongho/anaconda3/envs/temp/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/juhyeon/fsx/proj-medarc/fmri/natural-scenes-dataset/webdataset_avg_split/train/train_subj01_{0..17}.tar,/home/juhyeon/fsx/proj-medarc/fmri/natural-scenes-dataset/webdataset_avg_split/val/val_subj01_0.tar \n",
      " /home/juhyeon/fsx/proj-medarc/fmri/natural-scenes-dataset/webdataset_avg_split/test/test_subj01_{0..1}.tar\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import utils\n",
    "import webdataset as wds\n",
    "\n",
    "subj = 1\n",
    "\n",
    "data_path = \"/home/juhyeon/fsx/proj-medarc/fmri/natural-scenes-dataset\"\n",
    "\n",
    "\n",
    "train_url = f\"{data_path}/webdataset_avg_split/train/train_subj0{subj}_{{0..17}}.tar,{data_path}/webdataset_avg_split/val/val_subj0{subj}_0.tar\"\n",
    "val_url = f\"{data_path}/webdataset_avg_split/test/test_subj0{subj}_{{0..1}}.tar\"\n",
    "print(train_url,\"\\n\",val_url)\n",
    "meta_url = f\"{data_path}/webdataset_avg_split/metadata_subj0{subj}.csv\"\n",
    "num_train = 8559 + 300\n",
    "num_val = 982\n",
    "\n",
    "cache_dir=\"/tmp/wds-cache\"\n",
    "my_split_by_node = (lambda urls: urls)\n",
    "train_data = wds.WebDataset(train_url, resampled=True, cache_dir=cache_dir, nodesplitter=my_split_by_node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json \n",
    "# import pandas as pd\n",
    "\n",
    "# metadata = pd.read_csv(meta_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('Prepping train and validation dataloaders...')\n",
    "# train_dl, val_dl, num_train, num_val = utils.get_dataloaders(\n",
    "#     32,\n",
    "#     'images',\n",
    "#     num_devices=torch.cuda.device_count(),\n",
    "#     num_workers=1, \n",
    "#     train_url=train_url,\n",
    "#     val_url=val_url,\n",
    "#     meta_url=meta_url,\n",
    "#     num_train=num_train,\n",
    "#     num_val=num_val,\n",
    "#     val_batch_size=1,\n",
    "#     seed=0,\n",
    "#     voxels_key='nsdgeneral.npy',\n",
    "#     to_tuple=[\"voxels\", \"images\", \"coco\", \"brain_3d\"],\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# '''\n",
    "# voxels: (B, 3, 15724) # may vary slightly\n",
    "# image: (B, 3, 256, 256) \n",
    "# cocoid: (B, 1)\n",
    "# brain_3d: (B, 3, 81, 104, 83) # may vary slightly\n",
    "# '''\n",
    "\n",
    "# for i, (voxels, image, cocoid, brain_3d) in enumerate(train_dl):\n",
    "#     if (i==1): break\n",
    "#     print(f\"voxels.shape: {voxels.shape}\")\n",
    "#     print(f\"image.shape: {image.shape}\")\n",
    "#     print(f\"cocoid.shape: {cocoid.shape}\")\n",
    "#     print(f\"brain_3d.shape: {brain_3d.shape}\")\n",
    "\n",
    "#     repeat_index = i % 3\n",
    "#     for j in range(4):\n",
    "#         print(voxels[j, repeat_index])\n",
    "#         plt.figure(figsize=(3, 3))\n",
    "#         plt.imshow(utils.torch_to_Image(image[j]))\n",
    "#         plt.show()\n",
    "#         print(f\"cocoid: {cocoid[j]}\")\n",
    "#         print(brain_3d[j][repeat_index])\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모든 subject 대상 dataloader 만들기\n",
    "- cocoId로 EMOTIC과 겹치는 애들 뽑기\n",
    "- brain_3d and valence pair 만들기\n",
    "- train, val, test\n",
    "    - train, val = 8:2\n",
    "    - test: 공통 이미지 보여준 애들로"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Num of intersection between NSD and EMOTIC dataSet: 9413\n",
      "The number of joint pictures which reveals only one person: 7569\n",
      "cocoid for train, #: 7457\n",
      "cocoid for test, #: 112\n",
      "data of subject1: (960, 41)\n",
      "data of subject2: (900, 41)\n",
      "data of subject3: (910, 41)\n",
      "data of subject4: (968, 41)\n",
      "data of subject5: (967, 41)\n",
      "data of subject6: (932, 41)\n",
      "data of subject7: (903, 41)\n",
      "data of subject8: (917, 41)\n"
     ]
    }
   ],
   "source": [
    "# load package\n",
    "import numpy as np, pandas as pd, matplotlib.pyplot as plt, seaborn as sns\n",
    "import scipy.io\n",
    "\n",
    "file_name_nsd_stim = './nsd_stim_info_merged.csv'\n",
    "file_name_emotic_annot = './emotic_annotations.mat'\n",
    "\n",
    "## get EMOTIC data\n",
    "data = scipy.io.loadmat(file_name_emotic_annot, simplify_cells=True)\n",
    "emotic_data = data['train'] + data['test'] + data['val']\n",
    "emotic_coco_data = [x for x in emotic_data if x['original_database']['name']=='mscoco']\n",
    "coco_id = [x['original_database']['info']['image_id'] for x in emotic_coco_data]\n",
    "annotations = [x['person'] for x in emotic_coco_data] \n",
    "emotic_annotations = []\n",
    "for annot in annotations:\n",
    "    annot = [annot] if type(annot)==dict else annot\n",
    "\n",
    "    valence = []; arousal = []; dominance = []\n",
    "    for person in annot:\n",
    "        person = person['annotations_continuous']\n",
    "        person = [person] if type(person)==dict else person\n",
    "        valence += [np.mean([x['valence'] for x in person])]\n",
    "        arousal += [np.mean([x['arousal'] for x in person])]\n",
    "        dominance += [np.mean([x['dominance'] for x in person])]\n",
    "    emotic_annotations += [{ 'valence':valence, 'arousal':arousal, 'dominance':dominance}]\n",
    "\n",
    "emotic_annotations = dict(zip(coco_id, emotic_annotations))\n",
    "\n",
    "# define function\n",
    "get_emotic_annot       = lambda coco_id, metric_type: np.mean(emotic_annotations[coco_id][metric_type])\n",
    "get_emotic_annot_indiv = lambda coco_id, metric_type: emotic_annotations[coco_id][metric_type]\n",
    "\n",
    "## get NSD data\n",
    "df = data = pd.read_csv(file_name_nsd_stim)\n",
    "nsd_id = df['Unnamed: 0'].values\n",
    "nsd_cocoid = df['cocoId'].values\n",
    "nsd_cocosplit = df['cocoSplit'].values\n",
    "nsd_isshared = df['shared1000'].values\n",
    "\n",
    "\n",
    "## target NSD data\n",
    "joint_cocoid = nsd_cocoid[np.isin(nsd_cocoid, list(emotic_annotations.keys()))]\n",
    "# print(target_cocoid)\n",
    "print(\"The Num of intersection between NSD and EMOTIC dataSet:\", len(joint_cocoid))\n",
    "\n",
    "\"\"\"\n",
    "1. TrainSet: NSD 데이터셋에서 개별적으로 보여준 것\n",
    "2. TestSet: NSD 데이터셋에서 공통으로 보여준 것\n",
    "3. 공통으로, 사람 한명 있는 데이터 사용\n",
    " - `len(person)==1`\n",
    " \"\"\"\n",
    "# 사람 한명 있는 데이터\n",
    "target_cocoid = [coco_id for coco_id in joint_cocoid if len(get_emotic_annot_indiv(coco_id, 'valence')) == 1]\n",
    "print(\"The number of joint pictures which reveals only one person:\", len(target_cocoid))\n",
    "train_cocoid = nsd_cocoid[np.isin(nsd_cocoid, target_cocoid) &  ~nsd_isshared]\n",
    "print(f\"cocoid for train, #: {len(train_cocoid)}\")\n",
    "test_cocoid = nsd_cocoid[np.isin(nsd_cocoid, target_cocoid) &  nsd_isshared]\n",
    "print(f\"cocoid for test, #: {len(test_cocoid)}\")\n",
    "\n",
    "# data for common image shown to all subjects\n",
    "# regarding as test set\n",
    "test_df = df[df[['subject1', 'subject2', 'subject3', 'subject4', 'subject5', 'subject6', 'subject7', 'subject8']].all(axis=1)]\n",
    "train_df =  df[~df[['subject1', 'subject2', 'subject3', 'subject4', 'subject5', 'subject6', 'subject7', 'subject8']].all(axis=1)]\n",
    "\n",
    "for i in range(1, 9):\n",
    "    current_subject = f\"subject{i}\"\n",
    "    curr_df = train_df[train_df[current_subject] == True] # filter only shown to curr_subject\n",
    "\n",
    "    # 현재 subject의 데이터에서 train_cocoid와 겹치는 데이터 찾기\n",
    "    # 즉 현재 subject에 보여준 사진 중 cocoid가 EMOTIC dataset에 사용되었던 것 필터링\n",
    "    target_train_df = curr_df[np.isin(curr_df[\"cocoId\"], train_cocoid)]\n",
    "    print(f\"data of {current_subject}: {target_train_df.shape}\")\n",
    "\n",
    "target_test_df = test_df[np.isin(test_df[\"cocoId\"], test_cocoid)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_brain3d(brain_3d, target_shape=(96, 96, 96)):\n",
    "    # Initialize pad_width\n",
    "    pad_width = [(0, 0)] * 3  # For 3D brain_3day\n",
    "\n",
    "    # Calculate padding needed for each dimension\n",
    "    for i in range(3):\n",
    "        current_size = brain_3d.shape[i]\n",
    "        if current_size < target_shape[i]:\n",
    "            # Calculate padding\n",
    "            total_pad = target_shape[i] - current_size\n",
    "            pad_before = total_pad // 2\n",
    "            pad_after = total_pad - pad_before\n",
    "            pad_width[i] = (pad_before, pad_after)\n",
    "\n",
    "    # Apply padding\n",
    "    brain_3d = np.pad(brain_3d, pad_width=pad_width, mode='constant', constant_values=0)\n",
    "\n",
    "    # Apply truncation if necessary\n",
    "    brain_3d = brain_3d[:target_shape[0], :target_shape[1], :target_shape[2]]\n",
    "\n",
    "    # print(\"reshaped brain_3d shape\", brain_3d.shape)\n",
    "    return brain_3d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import braceexpand\n",
    "import math\n",
    "import random\n",
    "import webdataset as wds\n",
    "import pprint as pp\n",
    "\n",
    "def get_dataloaders(\n",
    "    batch_size,\n",
    "    target_cocoid,\n",
    "    image_var='images',\n",
    "    num_devices=None,\n",
    "    train_data_urls=None,\n",
    "    val_data_urls=None,\n",
    "    test_data_urls=None,\n",
    "    num_data=None,\n",
    "    seed=0,\n",
    "    voxels_key=\"nsdgeneral.npy\",\n",
    "    to_tuple=[\"voxels\", \"images\", \"coco\", \"brain_3d\"],\n",
    "):\n",
    "    \"\"\"\n",
    "    train_data_urls: array of train data url\n",
    "    val_data_urls: array of val data url\n",
    "    test_data_urls: array of test data url\n",
    "    target_cocoid(coco id from both NSD & EMOTIC dataset)\n",
    "    \n",
    "    out: three dataloaders that all returns (brain3d, valence)\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"Getting dataloaders...\")\n",
    "    assert image_var == 'images'\n",
    "    \n",
    "    def my_split_by_node(urls):\n",
    "        return urls\n",
    "    \n",
    "    # data_url = list(braceexpand.braceexpand(data_url))\n",
    "\n",
    "    if num_devices is None:\n",
    "        num_devices = torch.cuda.device_count()\n",
    "    \n",
    "        \n",
    "    print(f\"in utils.py: num_devices: {num_devices}\")\n",
    "    global_batch_size = batch_size * num_devices\n",
    "    num_batches = math.floor(num_data / global_batch_size)\n",
    "    # num_worker_batches = math.floor(num_batches / num_workers)\n",
    "    # if num_worker_batches == 0: num_worker_batches = 1\n",
    "\n",
    "    print(\"\\nnum_data\",num_data)\n",
    "    print(\"global_batch_size\",global_batch_size)\n",
    "    print(\"batch_size\",batch_size)\n",
    "    print(\"num_batches\",num_batches)\n",
    "\n",
    "    def filter_by_cocoId(sample):\n",
    "        # sample: (\"voxels\", \"images\", \"cocoid\", \"brain_3d\")\n",
    "        _, _, cocoid, _ = sample\n",
    "        cocoid = cocoid[-1]\n",
    "        return (cocoid in target_cocoid)\n",
    "\n",
    "    def map_brain_valence_pair(sample):\n",
    "        # sample: (\"voxels\", \"images\", \"cocoid\", \"brain_3d\")\n",
    "        # add corresponding valence\n",
    "        # make sure all brain_3d shape is same\n",
    "        # out: brain_3d, valence\n",
    "\n",
    "        _, _, cocoid, brain_3d = sample\n",
    "        cocoid = cocoid[-1]\n",
    "        valence = get_emotic_annot(cocoid, 'valence')\n",
    "        brain_3d = np.mean(brain_3d, axis=0) # (*, *, *)\n",
    "        brain_3d = reshape_brain3d(brain_3d, target_shape=(96, 96, 96)) # (96, 96, 96)\n",
    "        \n",
    "        return brain_3d, valence\n",
    "    \n",
    "    ### train ###\n",
    "    \n",
    "    train_target_urls = []\n",
    "    for url in train_data_urls:\n",
    "        train_target_urls += list(braceexpand.braceexpand(url))\n",
    "    print(\"len(train_target_urls):\", len(train_target_urls))\n",
    "\n",
    "    # for url in data_urls:\n",
    "    train_dataset = wds.WebDataset(train_target_urls, resampled=True, nodesplitter=my_split_by_node)\\\n",
    "        .shuffle(500, initial=500, rng=random.Random(seed))\\\n",
    "        .decode(\"torch\")\\\n",
    "        .rename(images=\"jpg;png\", voxels=voxels_key, trial=\"trial.npy\", coco=\"coco73k.npy\", reps=\"num_uniques.npy\", brain_3d = \"wholebrain_3d.npy\")\\\n",
    "        .to_tuple(*to_tuple)\\\n",
    "        .select(filter_by_cocoId)\\\n",
    "        .map(map_brain_valence_pair)\\\n",
    "        .batched(batch_size, partial=True)\\\n",
    "        # .with_epoch(num_worker_batches)\n",
    "\n",
    "    train_dl = torch.utils.data.DataLoader(train_dataset, batch_size=None, num_workers=1, shuffle=False)       \n",
    "\n",
    "    ### val ###\n",
    "    \n",
    "    val_target_urls = []\n",
    "    for url in val_data_urls:\n",
    "        val_target_urls += list(braceexpand.braceexpand(url))\n",
    "    print(\"len(val_target_urls):\", len(val_target_urls))\n",
    "\n",
    "    # for url in data_urls:\n",
    "    val_dataset = wds.WebDataset(val_target_urls, resampled=True, nodesplitter=my_split_by_node)\\\n",
    "        .shuffle(500, initial=500, rng=random.Random(seed))\\\n",
    "        .decode(\"torch\")\\\n",
    "        .rename(images=\"jpg;png\", voxels=voxels_key, trial=\"trial.npy\", coco=\"coco73k.npy\", reps=\"num_uniques.npy\", brain_3d = \"wholebrain_3d.npy\")\\\n",
    "        .to_tuple(*to_tuple)\\\n",
    "        .select(filter_by_cocoId)\\\n",
    "        .map(map_brain_valence_pair)\\\n",
    "        .batched(batch_size, partial=True)\\\n",
    "        # .with_epoch(num_worker_batches)\n",
    "\n",
    "    val_dl = torch.utils.data.DataLoader(val_dataset, batch_size=None, num_workers=1, shuffle=False)       \n",
    "\n",
    "    ### test ###\n",
    "    \n",
    "    test_target_urls = []\n",
    "    for url in test_data_urls:\n",
    "        test_target_urls += list(braceexpand.braceexpand(url))\n",
    "    print(\"len(test_target_urls):\", len(test_target_urls))\n",
    "\n",
    "    # for url in data_urls:\n",
    "    test_dataset = wds.WebDataset(test_target_urls, resampled=True, nodesplitter=my_split_by_node)\\\n",
    "        .shuffle(500, initial=500, rng=random.Random(seed))\\\n",
    "        .decode(\"torch\")\\\n",
    "        .rename(images=\"jpg;png\", voxels=voxels_key, trial=\"trial.npy\", coco=\"coco73k.npy\", reps=\"num_uniques.npy\", brain_3d = \"wholebrain_3d.npy\")\\\n",
    "        .to_tuple(*to_tuple)\\\n",
    "        .select(filter_by_cocoId)\\\n",
    "        .map(map_brain_valence_pair)\\\n",
    "        .batched(batch_size, partial=True)\\\n",
    "        # .with_epoch(num_worker_batches)\n",
    "\n",
    "    test_dl = torch.utils.data.DataLoader(test_dataset, batch_size=None, num_workers=1, shuffle=False)       \n",
    "    \n",
    "    return train_dl, val_dl, test_dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting dataloaders...\n",
      "in utils.py: num_devices: 4\n",
      "\n",
      "num_data 3730\n",
      "global_batch_size 128\n",
      "batch_size 32\n",
      "num_batches 29\n",
      "len(train_target_urls): 72\n",
      "len(val_target_urls): 4\n",
      "len(test_target_urls): 8\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<torch.utils.data.dataloader.DataLoader at 0x7f2e084484f0>,\n",
       " <torch.utils.data.dataloader.DataLoader at 0x7f2e07b98df0>,\n",
       " <torch.utils.data.dataloader.DataLoader at 0x7f2e555f67a0>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### load dataloaders\n",
    "\n",
    "data_path = \"/home/juhyeon/fsx/proj-medarc/fmri/natural-scenes-dataset\"\n",
    "\n",
    "train_urls = []\n",
    "for subj in [1, 2, 5, 7]:\n",
    "    train_url = f\"{data_path}/webdataset_avg_split/train/train_subj0{subj}_\" + \"{0..17}.tar\"\n",
    "    train_urls.append(train_url)\n",
    "\n",
    "    \n",
    "val_urls = []\n",
    "for subj in [1, 2, 5, 7]:\n",
    "    val_url = f\"{data_path}/webdataset_avg_split/val/val_subj0{subj}_0.tar\"\n",
    "    val_urls.append(val_url)\n",
    "\n",
    "test_urls = []\n",
    "for subj in [1, 2, 5, 7]:\n",
    "    test_url = f\"{data_path}/webdataset_avg_split/test/test_subj0{subj}_\" + \"{0..1}.tar\"\n",
    "    test_urls.append(test_url)\n",
    "    \n",
    "\n",
    "train_dl, val_dl, test_dl = get_dataloaders(\n",
    "    batch_size=32,\n",
    "    target_cocoid=target_cocoid,\n",
    "    num_devices=torch.cuda.device_count(),\n",
    "    train_data_urls=train_urls,\n",
    "    val_data_urls=val_urls,\n",
    "    test_data_urls=test_urls,\n",
    "    num_data=3730,\n",
    "    seed=0,\n",
    "    voxels_key='nsdgeneral.npy',\n",
    "    to_tuple=[\"voxels\", \"images\", \"coco\", \"brain_3d\"],\n",
    ")\n",
    "\n",
    "train_dl, val_dl, test_dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data in trainloader\n",
      "torch.Size([96, 96, 96])\n",
      "tensor(7., dtype=torch.float64)\n",
      "torch.Size([96, 96, 96])\n",
      "tensor(4., dtype=torch.float64)\n",
      "torch.Size([96, 96, 96])\n",
      "tensor(6., dtype=torch.float64)\n",
      "torch.Size([96, 96, 96])\n",
      "tensor(2., dtype=torch.float64)\n",
      "torch.Size([96, 96, 96])\n",
      "tensor(5.3333, dtype=torch.float64)\n",
      "data in valloader\n",
      "torch.Size([96, 96, 96])\n",
      "tensor(6.3333, dtype=torch.float64)\n",
      "torch.Size([96, 96, 96])\n",
      "tensor(5.5000, dtype=torch.float64)\n",
      "torch.Size([96, 96, 96])\n",
      "tensor(5.5000, dtype=torch.float64)\n",
      "torch.Size([96, 96, 96])\n",
      "tensor(5.5000, dtype=torch.float64)\n",
      "torch.Size([96, 96, 96])\n",
      "tensor(6.3333, dtype=torch.float64)\n",
      "data in testloader\n",
      "torch.Size([96, 96, 96])\n",
      "tensor(6.6000, dtype=torch.float64)\n",
      "torch.Size([96, 96, 96])\n",
      "tensor(8., dtype=torch.float64)\n",
      "torch.Size([96, 96, 96])\n",
      "tensor(6.6000, dtype=torch.float64)\n",
      "torch.Size([96, 96, 96])\n",
      "tensor(5.5000, dtype=torch.float64)\n",
      "torch.Size([96, 96, 96])\n",
      "tensor(6., dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "print(\"data in trainloader\")\n",
    "for i, (brain_3d, valence) in enumerate(train_dl):\n",
    "    if (i == 1): break\n",
    "    for j in range(5):\n",
    "        print(brain_3d[j].shape)\n",
    "        print(valence[j])\n",
    "print(\"data in valloader\")\n",
    "for i, (brain_3d, valence) in enumerate(val_dl):\n",
    "    if (i == 1): break\n",
    "    for j in range(5):\n",
    "        print(brain_3d[j].shape)\n",
    "        print(valence[j])\n",
    "print(\"data in testloader\")\n",
    "for i, (brain_3d, valence) in enumerate(test_dl):\n",
    "    if (i == 1): break\n",
    "    for j in range(5):\n",
    "        print(brain_3d[j].shape)\n",
    "        print(valence[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n",
      "64\n",
      "96\n",
      "128\n",
      "160\n",
      "192\n",
      "224\n",
      "256\n",
      "288\n",
      "320\n",
      "352\n",
      "384\n",
      "416\n",
      "448\n",
      "480\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f2e515a3ac0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/dongho/anaconda3/envs/temp/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1478, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/dongho/anaconda3/envs/temp/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1461, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/home/dongho/anaconda3/envs/temp/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f2e515a3ac0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/dongho/anaconda3/envs/temp/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1478, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/dongho/anaconda3/envs/temp/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1461, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/home/dongho/anaconda3/envs/temp/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512\n",
      "544\n",
      "576\n",
      "608\n",
      "640\n",
      "672\n",
      "704\n",
      "736\n",
      "768\n",
      "800\n",
      "832\n",
      "864\n",
      "896\n",
      "928\n",
      "960\n",
      "992\n",
      "1024\n",
      "1056\n",
      "1088\n",
      "1120\n",
      "1152\n",
      "1184\n",
      "1216\n",
      "1248\n",
      "1280\n",
      "1312\n",
      "1344\n",
      "1376\n",
      "1408\n",
      "1440\n",
      "1472\n",
      "1504\n",
      "1536\n",
      "1568\n",
      "1600\n",
      "1632\n",
      "1664\n",
      "1696\n",
      "1728\n",
      "1760\n",
      "1792\n",
      "1824\n",
      "1856\n",
      "1888\n",
      "1920\n",
      "1952\n",
      "1984\n",
      "2016\n",
      "2048\n",
      "2080\n",
      "2112\n",
      "2144\n",
      "2176\n",
      "2208\n",
      "2240\n",
      "2272\n",
      "2304\n",
      "2336\n",
      "2368\n",
      "2400\n",
      "2432\n",
      "2464\n",
      "2496\n",
      "2528\n",
      "2560\n",
      "2592\n",
      "2624\n",
      "2656\n",
      "2688\n",
      "2720\n",
      "2752\n",
      "2784\n",
      "2816\n",
      "2848\n",
      "2880\n",
      "2912\n",
      "2944\n",
      "2976\n",
      "3008\n",
      "3040\n",
      "3072\n",
      "3104\n",
      "3136\n",
      "3168\n",
      "3200\n",
      "3232\n",
      "3264\n",
      "3296\n",
      "3328\n",
      "3360\n",
      "3392\n",
      "3424\n",
      "3456\n",
      "3488\n",
      "3520\n",
      "3552\n",
      "3584\n",
      "3616\n",
      "3648\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# len(train_dl.dataset), len(val_dl.dataset), len(test_dl.dataset),\u001b[39;00m\n\u001b[1;32m      2\u001b[0m num_val_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, (brain_3d, valence) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(val_dl):\n\u001b[1;32m      4\u001b[0m     num_val_data \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m32\u001b[39m\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28mprint\u001b[39m(num_val_data)\n",
      "File \u001b[0;32m~/anaconda3/envs/temp/lib/python3.10/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/anaconda3/envs/temp/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1328\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1325\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_data(data)\n\u001b[1;32m   1327\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m-> 1328\u001b[0m idx, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1329\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1330\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable:\n\u001b[1;32m   1331\u001b[0m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/temp/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1294\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1290\u001b[0m     \u001b[38;5;66;03m# In this case, `self._data_queue` is a `queue.Queue`,. But we don't\u001b[39;00m\n\u001b[1;32m   1291\u001b[0m     \u001b[38;5;66;03m# need to call `.task_done()` because we don't use `.join()`.\u001b[39;00m\n\u001b[1;32m   1292\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1293\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m-> 1294\u001b[0m         success, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1295\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[1;32m   1296\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/anaconda3/envs/temp/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1132\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1119\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_try_get_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m_utils\u001b[38;5;241m.\u001b[39mMP_STATUS_CHECK_INTERVAL):\n\u001b[1;32m   1120\u001b[0m     \u001b[38;5;66;03m# Tries to fetch data from `self._data_queue` once for a given timeout.\u001b[39;00m\n\u001b[1;32m   1121\u001b[0m     \u001b[38;5;66;03m# This can also be used as inner loop of fetching without timeout, with\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1129\u001b[0m     \u001b[38;5;66;03m# Returns a 2-tuple:\u001b[39;00m\n\u001b[1;32m   1130\u001b[0m     \u001b[38;5;66;03m#   (bool: whether successfully get data, any: data if successful else None)\u001b[39;00m\n\u001b[1;32m   1131\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1132\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_queue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1133\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n\u001b[1;32m   1134\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1135\u001b[0m         \u001b[38;5;66;03m# At timeout and error, we manually check whether any worker has\u001b[39;00m\n\u001b[1;32m   1136\u001b[0m         \u001b[38;5;66;03m# failed. Note that this is the only mechanism for Windows to detect\u001b[39;00m\n\u001b[1;32m   1137\u001b[0m         \u001b[38;5;66;03m# worker failures.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/temp/lib/python3.10/multiprocessing/queues.py:113\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m block:\n\u001b[1;32m    112\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m deadline \u001b[38;5;241m-\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic()\n\u001b[0;32m--> 113\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_poll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    114\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_poll():\n",
      "File \u001b[0;32m~/anaconda3/envs/temp/lib/python3.10/multiprocessing/connection.py:262\u001b[0m, in \u001b[0;36m_ConnectionBase.poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    260\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_closed()\n\u001b[1;32m    261\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_readable()\n\u001b[0;32m--> 262\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_poll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/temp/lib/python3.10/multiprocessing/connection.py:429\u001b[0m, in \u001b[0;36mConnection._poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    428\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_poll\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout):\n\u001b[0;32m--> 429\u001b[0m     r \u001b[38;5;241m=\u001b[39m \u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    430\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mbool\u001b[39m(r)\n",
      "File \u001b[0;32m~/anaconda3/envs/temp/lib/python3.10/multiprocessing/connection.py:936\u001b[0m, in \u001b[0;36mwait\u001b[0;34m(object_list, timeout)\u001b[0m\n\u001b[1;32m    933\u001b[0m     deadline \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic() \u001b[38;5;241m+\u001b[39m timeout\n\u001b[1;32m    935\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 936\u001b[0m     ready \u001b[38;5;241m=\u001b[39m \u001b[43mselector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    937\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ready:\n\u001b[1;32m    938\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [key\u001b[38;5;241m.\u001b[39mfileobj \u001b[38;5;28;01mfor\u001b[39;00m (key, events) \u001b[38;5;129;01min\u001b[39;00m ready]\n",
      "File \u001b[0;32m~/anaconda3/envs/temp/lib/python3.10/selectors.py:416\u001b[0m, in \u001b[0;36m_PollLikeSelector.select\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    414\u001b[0m ready \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    415\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 416\u001b[0m     fd_event_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_selector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpoll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    417\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mInterruptedError\u001b[39;00m:\n\u001b[1;32m    418\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ready\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# len(train_dl.dataset), len(val_dl.dataset), len(test_dl.dataset),\n",
    "num_val_data = 0\n",
    "for i, (brain_3d, valence) in enumerate(val_dl):\n",
    "    num_val_data += 32\n",
    "    print(num_val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_url = f\"{data_path}/webdataset_avg_split/test/test_subj0{subj}_{{0..1}}.tar\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "temp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
