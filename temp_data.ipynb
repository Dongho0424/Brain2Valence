{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dongho/anaconda3/envs/temp/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/juhyeon/fsx/proj-medarc/fmri/natural-scenes-dataset/webdataset_avg_split/train/train_subj01_{0..17}.tar,/home/juhyeon/fsx/proj-medarc/fmri/natural-scenes-dataset/webdataset_avg_split/val/val_subj01_0.tar \n",
      " /home/juhyeon/fsx/proj-medarc/fmri/natural-scenes-dataset/webdataset_avg_split/test/test_subj01_{0..1}.tar\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import utils\n",
    "import webdataset as wds\n",
    "\n",
    "subj = 1\n",
    "\n",
    "data_path = \"/home/juhyeon/fsx/proj-medarc/fmri/natural-scenes-dataset\"\n",
    "\n",
    "\n",
    "train_url = f\"{data_path}/webdataset_avg_split/train/train_subj0{subj}_{{0..17}}.tar,{data_path}/webdataset_avg_split/val/val_subj0{subj}_0.tar\"\n",
    "val_url = f\"{data_path}/webdataset_avg_split/test/test_subj0{subj}_{{0..1}}.tar\"\n",
    "print(train_url,\"\\n\",val_url)\n",
    "meta_url = f\"{data_path}/webdataset_avg_split/metadata_subj0{subj}.csv\"\n",
    "num_train = 8559 + 300\n",
    "num_val = 982\n",
    "\n",
    "cache_dir=\"/tmp/wds-cache\"\n",
    "my_split_by_node = (lambda urls: urls)\n",
    "train_data = wds.WebDataset(train_url, resampled=True, cache_dir=cache_dir, nodesplitter=my_split_by_node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json \n",
    "# import pandas as pd\n",
    "\n",
    "# metadata = pd.read_csv(meta_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('Prepping train and validation dataloaders...')\n",
    "# train_dl, val_dl, num_train, num_val = utils.get_dataloaders(\n",
    "#     32,\n",
    "#     'images',\n",
    "#     num_devices=torch.cuda.device_count(),\n",
    "#     num_workers=1, \n",
    "#     train_url=train_url,\n",
    "#     val_url=val_url,\n",
    "#     meta_url=meta_url,\n",
    "#     num_train=num_train,\n",
    "#     num_val=num_val,\n",
    "#     val_batch_size=1,\n",
    "#     seed=0,\n",
    "#     voxels_key='nsdgeneral.npy',\n",
    "#     to_tuple=[\"voxels\", \"images\", \"coco\", \"brain_3d\"],\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# '''\n",
    "# voxels: (B, 3, 15724) # may vary slightly\n",
    "# image: (B, 3, 256, 256) \n",
    "# cocoid: (B, 1)\n",
    "# brain_3d: (B, 3, 81, 104, 83) # may vary slightly\n",
    "# '''\n",
    "\n",
    "# for i, (voxels, image, cocoid, brain_3d) in enumerate(train_dl):\n",
    "#     if (i==1): break\n",
    "#     print(f\"voxels.shape: {voxels.shape}\")\n",
    "#     print(f\"image.shape: {image.shape}\")\n",
    "#     print(f\"cocoid.shape: {cocoid.shape}\")\n",
    "#     print(f\"brain_3d.shape: {brain_3d.shape}\")\n",
    "\n",
    "#     repeat_index = i % 3\n",
    "#     for j in range(4):\n",
    "#         print(voxels[j, repeat_index])\n",
    "#         plt.figure(figsize=(3, 3))\n",
    "#         plt.imshow(utils.torch_to_Image(image[j]))\n",
    "#         plt.show()\n",
    "#         print(f\"cocoid: {cocoid[j]}\")\n",
    "#         print(brain_3d[j][repeat_index])\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모든 subject 대상 dataloader 만들기\n",
    "- cocoId로 EMOTIC과 겹치는 애들 뽑기\n",
    "- brain_3d and valence pair 만들기\n",
    "- train, val, test\n",
    "    - train, val = 8:2\n",
    "    - test: 공통 이미지 보여준 애들로"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Num of intersection between NSD and EMOTIC dataSet: 9413\n",
      "The number of joint pictures which reveals only one person: 7569\n",
      "cocoid for train, #: 7457\n",
      "cocoid for test, #: 112\n",
      "data of subject1: (960, 41)\n",
      "data of subject2: (900, 41)\n",
      "data of subject3: (910, 41)\n",
      "data of subject4: (968, 41)\n",
      "data of subject5: (967, 41)\n",
      "data of subject6: (932, 41)\n",
      "data of subject7: (903, 41)\n",
      "data of subject8: (917, 41)\n"
     ]
    }
   ],
   "source": [
    "# load package\n",
    "import numpy as np, pandas as pd, matplotlib.pyplot as plt, seaborn as sns\n",
    "import scipy.io\n",
    "\n",
    "file_name_nsd_stim = './nsd_stim_info_merged.csv'\n",
    "file_name_emotic_annot = './emotic_annotations.mat'\n",
    "\n",
    "## get EMOTIC data\n",
    "data = scipy.io.loadmat(file_name_emotic_annot, simplify_cells=True)\n",
    "emotic_data = data['train'] + data['test'] + data['val']\n",
    "emotic_coco_data = [x for x in emotic_data if x['original_database']['name']=='mscoco']\n",
    "coco_id = [x['original_database']['info']['image_id'] for x in emotic_coco_data]\n",
    "annotations = [x['person'] for x in emotic_coco_data] \n",
    "emotic_annotations = []\n",
    "for annot in annotations:\n",
    "    annot = [annot] if type(annot)==dict else annot\n",
    "\n",
    "    valence = []; arousal = []; dominance = []\n",
    "    for person in annot:\n",
    "        person = person['annotations_continuous']\n",
    "        person = [person] if type(person)==dict else person\n",
    "        valence += [np.mean([x['valence'] for x in person])]\n",
    "        arousal += [np.mean([x['arousal'] for x in person])]\n",
    "        dominance += [np.mean([x['dominance'] for x in person])]\n",
    "    emotic_annotations += [{ 'valence':valence, 'arousal':arousal, 'dominance':dominance}]\n",
    "\n",
    "emotic_annotations = dict(zip(coco_id, emotic_annotations))\n",
    "\n",
    "# define function\n",
    "get_emotic_annot       = lambda coco_id, metric_type: np.mean(emotic_annotations[coco_id][metric_type])\n",
    "get_emotic_annot_indiv = lambda coco_id, metric_type: emotic_annotations[coco_id][metric_type]\n",
    "\n",
    "## get NSD data\n",
    "df = data = pd.read_csv(file_name_nsd_stim)\n",
    "nsd_id = df['Unnamed: 0'].values\n",
    "nsd_cocoid = df['cocoId'].values\n",
    "nsd_cocosplit = df['cocoSplit'].values\n",
    "nsd_isshared = df['shared1000'].values\n",
    "\n",
    "\n",
    "## target NSD data\n",
    "joint_cocoid = nsd_cocoid[np.isin(nsd_cocoid, list(emotic_annotations.keys()))]\n",
    "# print(target_cocoid)\n",
    "print(\"The Num of intersection between NSD and EMOTIC dataSet:\", len(joint_cocoid))\n",
    "\n",
    "\"\"\"\n",
    "1. TrainSet: NSD 데이터셋에서 개별적으로 보여준 것\n",
    "2. TestSet: NSD 데이터셋에서 공통으로 보여준 것\n",
    "3. 공통으로, 사람 한명 있는 데이터 사용\n",
    " - `len(person)==1`\n",
    " \"\"\"\n",
    "# 사람 한명 있는 데이터\n",
    "target_cocoid = [coco_id for coco_id in joint_cocoid if len(get_emotic_annot_indiv(coco_id, 'valence')) == 1]\n",
    "print(\"The number of joint pictures which reveals only one person:\", len(target_cocoid))\n",
    "train_cocoid = nsd_cocoid[np.isin(nsd_cocoid, target_cocoid) &  ~nsd_isshared]\n",
    "print(f\"cocoid for train, #: {len(train_cocoid)}\")\n",
    "test_cocoid = nsd_cocoid[np.isin(nsd_cocoid, target_cocoid) &  nsd_isshared]\n",
    "print(f\"cocoid for test, #: {len(test_cocoid)}\")\n",
    "\n",
    "# data for common image shown to all subjects\n",
    "# regarding as test set\n",
    "test_df = df[df[['subject1', 'subject2', 'subject3', 'subject4', 'subject5', 'subject6', 'subject7', 'subject8']].all(axis=1)]\n",
    "train_df =  df[~df[['subject1', 'subject2', 'subject3', 'subject4', 'subject5', 'subject6', 'subject7', 'subject8']].all(axis=1)]\n",
    "\n",
    "for i in range(1, 9):\n",
    "    current_subject = f\"subject{i}\"\n",
    "    curr_df = train_df[train_df[current_subject] == True] # filter only shown to curr_subject\n",
    "\n",
    "    # 현재 subject의 데이터에서 train_cocoid와 겹치는 데이터 찾기\n",
    "    # 즉 현재 subject에 보여준 사진 중 cocoid가 EMOTIC dataset에 사용되었던 것 필터링\n",
    "    target_train_df = curr_df[np.isin(curr_df[\"cocoId\"], train_cocoid)]\n",
    "    print(f\"data of {current_subject}: {target_train_df.shape}\")\n",
    "\n",
    "target_test_df = test_df[np.isin(test_df[\"cocoId\"], test_cocoid)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_brain3d(brain_3d, target_shape=(96, 96, 96)):\n",
    "    # Initialize pad_width\n",
    "    pad_width = [(0, 0)] * 3  # For 3D brain_3day\n",
    "\n",
    "    # Calculate padding needed for each dimension\n",
    "    for i in range(3):\n",
    "        current_size = brain_3d.shape[i]\n",
    "        if current_size < target_shape[i]:\n",
    "            # Calculate padding\n",
    "            total_pad = target_shape[i] - current_size\n",
    "            pad_before = total_pad // 2\n",
    "            pad_after = total_pad - pad_before\n",
    "            pad_width[i] = (pad_before, pad_after)\n",
    "\n",
    "    # Apply padding\n",
    "    brain_3d = np.pad(brain_3d, pad_width=pad_width, mode='constant', constant_values=0)\n",
    "\n",
    "    # Apply truncation if necessary\n",
    "    brain_3d = brain_3d[:target_shape[0], :target_shape[1], :target_shape[2]]\n",
    "\n",
    "    # print(\"reshaped brain_3d shape\", brain_3d.shape)\n",
    "    return brain_3d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting dataloaders...\n",
      "in utils.py: num_devices: 4\n",
      "\n",
      "num_data 3730\n",
      "global_batch_size 128\n",
      "batch_size 32\n",
      "num_batches 29\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<webdataset.compat.WebDataset at 0x7f87f05a0a30>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import braceexpand\n",
    "import math\n",
    "import random\n",
    "import webdataset as wds\n",
    "import pprint as pp\n",
    "\n",
    "def get_dataloaders(\n",
    "    batch_size,\n",
    "    target_cocoid,\n",
    "    image_var='images',\n",
    "    num_devices=None,\n",
    "    data_urls=None,\n",
    "    num_data=None,\n",
    "    seed=0,\n",
    "    voxels_key=\"nsdgeneral.npy\",\n",
    "    to_tuple=[\"voxels\", \"images\", \"coco\", \"brain_3d\"],\n",
    "):\n",
    "    \"\"\"\n",
    "    url에 따른 dataloader return\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"Getting dataloaders...\")\n",
    "    assert image_var == 'images'\n",
    "    \n",
    "    def my_split_by_node(urls):\n",
    "        return urls\n",
    "    \n",
    "    # data_url = list(braceexpand.braceexpand(data_url))\n",
    "\n",
    "    if num_devices is None:\n",
    "        num_devices = torch.cuda.device_count()\n",
    "    \n",
    "        \n",
    "    print(f\"in utils.py: num_devices: {num_devices}\")\n",
    "    global_batch_size = batch_size * num_devices\n",
    "    num_batches = math.floor(num_data / global_batch_size)\n",
    "    # num_worker_batches = math.floor(num_batches / num_workers)\n",
    "    # if num_worker_batches == 0: num_worker_batches = 1\n",
    "\n",
    "    print(\"\\nnum_data\",num_data)\n",
    "    print(\"global_batch_size\",global_batch_size)\n",
    "    print(\"batch_size\",batch_size)\n",
    "    print(\"num_batches\",num_batches)\n",
    "\n",
    "    def filter_by_cocoId(sample):\n",
    "        # sample: (\"voxels\", \"images\", \"cocoid\", \"brain_3d\")\n",
    "        _, _, cocoid, _ = sample\n",
    "        cocoid = cocoid[-1]\n",
    "        return (cocoid in target_cocoid)\n",
    "\n",
    "    def map_brain_valence_pair(sample):\n",
    "        # sample: (\"voxels\", \"images\", \"cocoid\", \"brain_3d\")\n",
    "        # add corresponding valence\n",
    "        # make sure all brain_3d shape is same\n",
    "        # out: brain_3d, valence\n",
    "\n",
    "        _, _, cocoid, brain_3d = sample\n",
    "        cocoid = cocoid[-1]\n",
    "        valence = get_emotic_annot(cocoid, 'valence')\n",
    "        brain_3d = np.mean(brain_3d, axis=0) # (*, *, *)\n",
    "        brain_3d = reshape_brain3d(brain_3d, target_shape=(96, 96, 96)) # (96, 96, 96)\n",
    "        \n",
    "        return brain_3d, valence\n",
    "    \n",
    "    target_urls = []\n",
    "    for url in data_urls:\n",
    "        target_urls += list(braceexpand.braceexpand(url))\n",
    "\n",
    "    # for url in data_urls:\n",
    "    dataset = wds.WebDataset(target_urls, resampled=True, nodesplitter=my_split_by_node)\\\n",
    "        .shuffle(500, initial=500, rng=random.Random(seed))\\\n",
    "        .decode(\"torch\")\\\n",
    "        .rename(images=\"jpg;png\", voxels=voxels_key, trial=\"trial.npy\", coco=\"coco73k.npy\", reps=\"num_uniques.npy\", brain_3d = \"wholebrain_3d.npy\")\\\n",
    "        .to_tuple(*to_tuple)\\\n",
    "        .select(filter_by_cocoId)\\\n",
    "        .map(map_brain_valence_pair)\\\n",
    "        .batched(batch_size, partial=True)\\\n",
    "        # .with_epoch(num_worker_batches)\n",
    "\n",
    "    train_dl = torch.utils.data.DataLoader(dataset, batch_size=None, num_workers=1, shuffle=False)       \n",
    "    \n",
    "    return train_dl\n",
    "\n",
    "data_path = \"/home/juhyeon/fsx/proj-medarc/fmri/natural-scenes-dataset\"\n",
    "\n",
    "train_urls = []\n",
    "for subj in [1, 2, 5, 7]:\n",
    "    train_url = \"{\" + f\"{data_path}/webdataset_avg_split/train/train_subj0{subj}_\" + \"{0..17}.tar,\" + f\"{data_path}/webdataset_avg_split/val/val_subj0{subj}_0.tar\" + \"}\"\n",
    "    train_urls.append(train_url)\n",
    "    \n",
    "\n",
    "train_dl = get_dataloaders(\n",
    "    batch_size=32,\n",
    "    target_cocoid=target_cocoid,\n",
    "    num_devices=torch.cuda.device_count(),\n",
    "    data_urls=train_urls,\n",
    "    num_data=3730,\n",
    "    seed=0,\n",
    "    voxels_key='nsdgeneral.npy',\n",
    "    to_tuple=[\"voxels\", \"images\", \"coco\", \"brain_3d\"],\n",
    ")\n",
    "\n",
    "train_dl.dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([96, 96, 96])\n",
      "tensor(4., dtype=torch.float64)\n",
      "torch.Size([96, 96, 96])\n",
      "tensor(5.6667, dtype=torch.float64)\n",
      "torch.Size([96, 96, 96])\n",
      "tensor(7., dtype=torch.float64)\n",
      "torch.Size([96, 96, 96])\n",
      "tensor(6., dtype=torch.float64)\n",
      "torch.Size([96, 96, 96])\n",
      "tensor(5.3333, dtype=torch.float64)\n",
      "torch.Size([96, 96, 96])\n",
      "tensor(6.6000, dtype=torch.float64)\n",
      "torch.Size([96, 96, 96])\n",
      "tensor(8., dtype=torch.float64)\n",
      "torch.Size([96, 96, 96])\n",
      "tensor(5., dtype=torch.float64)\n",
      "torch.Size([96, 96, 96])\n",
      "tensor(6., dtype=torch.float64)\n",
      "torch.Size([96, 96, 96])\n",
      "tensor(5., dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "for i, (brain_3d, valence) in enumerate(train_dl):\n",
    "    if (i == 1): break\n",
    "    for j in range(10):\n",
    "        print(brain_3d[j].shape)\n",
    "        print(valence[j])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_url = f\"{data_path}/webdataset_avg_split/test/test_subj0{subj}_{{0..1}}.tar\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "temp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
